{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinforcement Learning \n",
    "\n",
    "Reinforcement Learning (RL) is an area of machine learning:\n",
    "* how do software agents take action in an environment\n",
    "* to MAXIMASE cumulative rewared \n",
    "\n",
    "OR teaching a software agent how to behave in an environment by telling it how well it's doing \n",
    "\n",
    "We use DEEP Q Learning \n",
    "\n",
    "Uses a *deep neural network* to _predict_ the actions\n",
    "\n",
    "![Overview of Model](Images/Overview.png)\n",
    "\n",
    "Agent \n",
    "\n",
    "\n",
    "Agent must be aware of the game and the model \n",
    "It then runs a training loop: \n",
    "1. Based on the Game we calculate a state \n",
    "2. Based on the State we calculate the next action \n",
    "    * Involved calling the model to make a prediction \n",
    "3. We then collect (reward, game over state ,score) from inserting the action from (Step 2)\n",
    "4. We make a new state \n",
    "5. Remember both the new state and the old state \n",
    "6. We then train the model \n",
    "\n",
    "Reward: \n",
    "Eat food : + 10 \n",
    "Game end : -10\n",
    "Else     : 0\n",
    "\n",
    "Action:\n",
    "Straight    : [ 1 , 0 , 0 ]\n",
    "Right Turn  : [ 0 , 1 , 0 ]\n",
    "Left Turn   : [ 0 , 0 , 1 ]\n",
    "\n",
    "<H3> State: </H3>\n",
    "\n",
    "(Tell the snake information about the game/ Environment )\n",
    "\n",
    "11 values: \n",
    "(Appears only when snake is next to wall )\n",
    "* Wall is to the left \n",
    "* Wall is to the right\n",
    "* Wall is straight ahead \n",
    "\n",
    "Current direction of the snake:\n",
    "* UP, DOWN , LEFT , RIGHT \n",
    "\n",
    "Relative location of food to snake: \n",
    "* FOOD IS UP\n",
    "* FOOD IS DOWN \n",
    "* FOOD IS LEFT \n",
    "* FOOD IS RIGHT \n",
    "\n",
    "Model\n",
    "\n",
    "![Model Visualisation](Images/Model.png)\n",
    "\n",
    "Model contains an *INPUT LAYER* a *HIDDEN LAYER* and an *OUTPUT LAYER* \n",
    "Input has 11 different numbers (all Boolean (0,1))\n",
    "From the output we decide the action (3 outputs)\n",
    "\n",
    "Output will be raw numbers (we take the maximum)\n",
    " \n",
    "Action decides which way the snake is turning \n",
    "[5,3,1] -> [1,0,0] (go Straight)\n",
    "\n",
    "\n",
    "<H3> DeepQLearning </H3>\n",
    "Q Value (Quality of action)\n",
    "\n",
    "![DeepQImage](Images/DeepQLearning.png)\n",
    "\n",
    "Each action should improve the quality of the snake. \n",
    "\n",
    "0. Initialise the Q value (initialise the model with random parameters)\n",
    "1. We choose an action (with ```model.predict(state)```)\n",
    "    * Early in the game we may produce a random move when we don't know a lot about the game,\n",
    "     eventually we will trade off when we don't want to randomise the actions but call ```model.predict(state)``` instead\n",
    "     This is referred to as a trade off between *Exploration* and *Exploitation* \n",
    "2. With new action we perform it \n",
    "3. Measure the reward \n",
    "4. Update the Q value (this trains the model)\n",
    "\n",
    "REPEATING STEPS 1-4 [ITERATIVE TRAINING LOOP]\n",
    "\n",
    "To train a model we need to have a Loss function we need to optimise \n",
    "For the Loss function we will use Bellman Equation: \n",
    "\n",
    "![BellManEquation](Images/BellmanEquation.png)\n",
    "\n",
    "We want to update the Q value \n",
    "\n",
    "New Q value is the current Q value + Learning Rate x [(Reward for taking action at that state) + Discount Rate x (Maximum expected future reward given a new state and all possible actions at that new state) - Current Q value]\n",
    "\n",
    "![QNewState](Images/Qnew.png)\n",
    "\n",
    "Q0 (First Q value) is when the model predicts a state of random parameters (State0)\n",
    "QNew  is Reward + (Gamma = Discount Rate )  x Maximum Value of Q State (at State1)\n",
    "\n",
    "Loss = (QNew - Q0)^2 (Means squared errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "We create 3 classes \n",
    "One for Game \n",
    "One for Model \n",
    "One for Agent\n",
    "\n",
    "\n",
    "```source game_env/bin/activate```\n",
    "\n",
    "Going through snake_game.py file \n",
    "\n",
    "Enum\n",
    "\n",
    "```from enum import Enum```\n",
    "\n",
    "enum is used by using a class syntax \n",
    "[DOCUMENTATION](https://docs.python.org/3/library/enum.html)\n",
    "[VIDEO](https://youtu.be/GiAHicNFvBU)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State.PLAYING\n",
      "MessageType.CHAT_ENDED_EVENT\n"
     ]
    }
   ],
   "source": [
    "from enum import Enum \n",
    "\n",
    "class State(Enum):\n",
    "    PLAYING = 0 \n",
    "    PAUSED = 1 \n",
    "    GAME_OVER = 2\n",
    "    \"Values don't matter we can use auto() instead of 1/2/3 for it to set an automatic value\"\n",
    "\n",
    "print(State.PLAYING)\n",
    "state = State.PLAYING\n",
    "\n",
    "\"Different example of use for Enum\"\n",
    "class MessageType(Enum):\n",
    "    CHAT_ENDED_EVENT = \"ChatEndedEvent\"\n",
    "    MESSAGE_DELETED_EVENT = \"messageDeletedEvent\"\n",
    "    NEW_SPONSOR_EVENT = \"newSponsorEvent\"\n",
    "    SPONSOR_ONLY_MODE_ENDED_EVENT = \"sponsorOnlyModeEndedEvent\"\n",
    "    SPONSOR_ONLY_MODE_STARTED_EVENT = \"sponsorOnlyModeStartedEvent\"\n",
    "    MEMBER_MILESTONE_CHAT_EVENT = \"memberMilestoneChatEvent\"\n",
    "    SUPER_CHAT_EVENT = \"superChatEvent\"\n",
    "    SUPER_STICKER_EVENT = \"superStickerEvent\"\n",
    "    TEXT_MESSAGE_EVENT = \"textMessageEvent\"\n",
    "    TOMBSTONE = \"tombstone\"\n",
    "    USER_BANNED_EVENT = \"userBannedEvent\"\n",
    "\n",
    "value = \"ChatEndedEvent\"\n",
    "print(MessageType(value))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Named Tuple\n",
    "[Video Explanation](https://www.youtube.com/watch?v=GfxJYp9_nJA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal tuple call:  55\n",
      "Using namedtuple function:  55\n",
      "The blue (in the RBG) value of white : 255\n"
     ]
    }
   ],
   "source": [
    "\"Instead of doing this: \"\n",
    "colour = (55,155,255)\n",
    "print('Normal tuple call: ',colour[0])\n",
    "\n",
    "\"We could do this\"\n",
    "colour = {'red':55,'green':155,'blue':255}\n",
    "\n",
    "\"\"\"But we can change values, dictionary require typing \n",
    "but a named tuple\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "----------------------------------------------------------------------------------------------\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from collections import namedtuple\n",
    "\n",
    "Colour = namedtuple('Colour',['red','green','blue'])\n",
    "colour = Colour(55,155,255)\n",
    "\n",
    "print ('Using namedtuple function: ',colour.red)\n",
    "\n",
    "\"Allows for less typing because you can add more colour:\"\n",
    "white  = Colour(255,255,255)\n",
    "\n",
    "print('The blue (in the RBG) value of white :', white.blue)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python Class refresher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corey.Schafer@company.com\n",
      "Corey Schafer\n"
     ]
    }
   ],
   "source": [
    "class Employee:\n",
    "\n",
    "    def __init__(self,first,last,pay):\n",
    "        self.first = first \n",
    "        self.last = last \n",
    "        self.pay = pay \n",
    "        self.email = first + '.' + last + '@company.com'\n",
    "\n",
    "\n",
    "    \"\"\"Don't forget use of self where appropriate\"\"\"\n",
    "    def full_name(self):\n",
    "        return ( self.first + ' ' + self.last)\n",
    "\n",
    "\n",
    "emp_1 = Employee('Corey','Schafer',50000)\n",
    "emp_2 = Employee('Test','User',60000)\n",
    "\n",
    "print(emp_1.email)\n",
    "print(emp_1.full_name())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agent intiated \n",
    "%Talk about deque (watch Youtube video\n",
    "https://youtu.be/fCbyMBnyfB8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[PART 3](https://www.freecodecamp.org/news/train-an-ai-to-play-a-snake-game-using-python/)\n",
    "40:15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pygame_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
